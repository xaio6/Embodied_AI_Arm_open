"""
æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å¤„ç†å™¨

é›†æˆASRè¯­éŸ³è¯†åˆ«ã€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹è¯å’ŒTTSè¯­éŸ³åˆæˆçš„å®Œæ•´æ™ºèƒ½è¯­éŸ³äº¤äº’åŠŸèƒ½
"""

from typing import Dict, Any, List
import time
import logging

logger = logging.getLogger(__name__)

class SmartMultiModalVoiceChatHandler:
    """
    æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å¤„ç†å™¨ç±»
    
    ç®¡ç†éº¦å…‹é£è¾“å…¥ã€è¯­éŸ³è¯†åˆ«ã€å¤šæ¨¡æ€LLMå¯¹è¯å’Œè¯­éŸ³å›å¤çš„å®Œæ•´æµç¨‹
    
    åŠŸèƒ½ï¼š
    - æ”¯æŒå…³é”®è¯å”¤é†’
    - éº¦å…‹é£è¯­éŸ³å½•å…¥
    - ä¸Šä¼ å›¾åƒ/è§†é¢‘è¿›è¡Œå¤šæ¨¡æ€åˆ†æ
    - æµå¼LLMå¤„ç†
    - å®æ—¶è¯­éŸ³åˆæˆæ’­æ”¾
    - ä¸Šä¸‹æ–‡å¯¹è¯æ”¯æŒ
    - è¯­éŸ³æŒ‡ä»¤æ§åˆ¶
    """
    
    def __init__(self, sdk):
        """
        åˆå§‹åŒ–æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å¤„ç†å™¨
        
        Args:
            sdk: AI SDKå®ä¾‹
        """
        self.sdk = sdk
    
    def handle_multimodal_voice_chat(self,
                           image_path: str = None,
                           video_path: str = None,
                           image_paths: List[str] = None,
                           duration: int = 5,
                           llm_provider: str = "alibaba",
                           llm_model: str = "qwen-vl-max-latest",
                           tts_provider: str = "alibaba",
                           tts_model: str = "sambert-zhichu-v1",
                           use_context: bool = True,
                           session_id: str = "multimodal_voice_chat",
                           continue_conversation: bool = True,
                           activation_phrase: str = "ä½ å¥½åŠ©æ‰‹",
                           activate_once: bool = True,
                           end_phrase: str = "ç»“æŸå¯¹è¯",
                           silence_timeout: float = 2.0,
                           verbose: bool = False,
                           **kwargs) -> Dict[str, Any]:
        """
        å¤„ç†å®Œæ•´çš„å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯æµç¨‹
        
        Args:
            image_path: å›¾åƒè·¯å¾„ï¼Œå¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶æˆ–URL
            video_path: è§†é¢‘è·¯å¾„ï¼Œå¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶æˆ–URL
            image_paths: å¤šå›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œç”¨äºæ¯”è¾ƒå¤šå¼ å›¾åƒ
            duration: æ¯æ¬¡å½•éŸ³çš„æœ€å¤§ç§’æ•°ï¼Œé»˜è®¤5ç§’
            llm_provider: LLMæä¾›å•†åç§°ï¼Œé»˜è®¤"alibaba"
            llm_model: LLMæ¨¡å‹åç§°ï¼Œé»˜è®¤"qwen-vl-max-latest"
            tts_provider: TTSæä¾›å•†åç§°ï¼Œé»˜è®¤"alibaba"
            tts_model: TTSæ¨¡å‹åç§°ï¼Œé»˜è®¤"sambert-zhichu-v1"
            use_context: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å¯¹è¯ï¼Œé»˜è®¤True
            session_id: ä¼šè¯IDï¼Œé»˜è®¤"multimodal_voice_chat"
            continue_conversation: æ˜¯å¦æŒç»­å¯¹è¯ï¼Œé»˜è®¤True
            activation_phrase: æ¿€æ´»çŸ­è¯­ï¼Œè¯´å‡ºæ­¤çŸ­è¯­å¼€å§‹å¯¹è¯ï¼Œä¸ºNoneæ—¶ä¸éœ€è¦æ¿€æ´»çŸ­è¯­
            activate_once: æ˜¯å¦åªéœ€æ¿€æ´»ä¸€æ¬¡ï¼Œé»˜è®¤Trueï¼Œå³é¦–æ¬¡å¯åŠ¨å¯¹è¯æ—¶éœ€è¦æ¿€æ´»ï¼Œåç»­å¯¹è¯ä¸éœ€è¦
            end_phrase: ç»“æŸå¯¹è¯çš„çŸ­è¯­ï¼Œé»˜è®¤"ç»“æŸå¯¹è¯"
            silence_timeout: é™éŸ³è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œæ£€æµ‹åˆ°é™éŸ³è¿™ä¹ˆé•¿æ—¶é—´åè®¤ä¸ºè¯­éŸ³è¾“å…¥ç»“æŸ
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼Œé»˜è®¤False
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬LLMå’ŒTTSçš„å‚æ•°
            
        Returns:
            Dict[str, Any]: å¯¹è¯ç»“æœä¿¡æ¯
        """
        # ä½¿ç”¨ç®€åŒ–å®ç°ï¼šç»„åˆç°æœ‰åŠŸèƒ½
        result = {
            "success": True,
            "conversations": []
        }
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†å¤šæ¨¡æ€åª’ä½“
        if not any([image_path, video_path, image_paths]):
            error_msg = "å¿…é¡»æä¾›å›¾åƒè·¯å¾„(image_path)ã€è§†é¢‘è·¯å¾„(video_path)æˆ–å¤šå›¾åƒè·¯å¾„(image_paths)ä¸­çš„è‡³å°‘ä¸€é¡¹"
            if verbose:
                logger.error(error_msg)
            print(f"âŒ é”™è¯¯: {error_msg}")
            return {"success": False, "error": error_msg}
            
        # ç¡®å®šå¤šæ¨¡æ€åª’ä½“ç±»å‹
        if image_paths:
            media_type = "å¤šå›¾åƒ"
            media_info = f"{len(image_paths)}å¼ å›¾ç‰‡"
        elif video_path:
            media_type = "è§†é¢‘"
            media_info = f"{video_path}"
        elif image_path:
            media_type = "å›¾åƒ"
            media_info = f"{image_path}"
        
        if verbose:
            logger.info("æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å·²å¯åŠ¨...")
        print(f"ğŸ™ï¸ğŸ–¼ï¸ æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å·²å¯åŠ¨... ({media_type}: {media_info})")
        
        # æå–TTSå‚æ•°
        voice = kwargs.pop('voice', None)
        
        # ä»kwargsä¸­æå–é™é»˜å‚æ•°
        tts_kwargs = kwargs.copy()
        if not verbose:
            tts_kwargs['silent'] = True
        
        conversation_active = True
        waiting_for_activation = activation_phrase is not None
        activated_once = False
        
        try:
            while conversation_active:
                # æ¿€æ´»å¤„ç†
                if waiting_for_activation and (not activate_once or not activated_once):
                    if verbose:
                        logger.info(f"ç­‰å¾…æ¿€æ´»çŸ­è¯­: '{activation_phrase}'...")
                    print(f"ç­‰å¾…æ¿€æ´»çŸ­è¯­: '{activation_phrase}'...")
                    
                    # ä½¿ç”¨ASRå…³é”®è¯æ¨¡å¼ç­‰å¾…æ¿€æ´»
                    for keyword_result in self.sdk.asr(
                        provider=llm_provider,
                        mode="keyword",
                        keywords=[activation_phrase],
                        detection_threshold=0.6
                    ):
                        if keyword_result.get('success') and keyword_result.get('keyword_detected'):
                            if verbose:
                                logger.info(f"å·²æ¿€æ´»! æ£€æµ‹åˆ°: '{keyword_result['keyword_detected']}'")
                            print(f"âœ“ å·²æ¿€æ´»! æ£€æµ‹åˆ°: '{keyword_result['keyword_detected']}'")
                            waiting_for_activation = False
                            activated_once = True
                            break
                
                # å·²æ¿€æ´»ï¼Œå¼€å§‹è¯­éŸ³è¾“å…¥
                print("ğŸ¤ æ­£åœ¨è†å¬...(è¯­éŸ³åœæ­¢æˆ–è¯´å‡ºç»“æŸçŸ­è¯­å°†å‘é€å¯¹è¯)")
                
                # ä½¿ç”¨ASRè·å–ç”¨æˆ·è¯­éŸ³è¾“å…¥
                user_input = ""
                
                # é¢„å¤„ç†ç»“æŸçŸ­è¯­
                import re
                def clean_text(text):
                    if not text:
                        return ""
                    cleaned = re.sub(r'[^\w\s]', '', text.lower())
                    cleaned = ' '.join(cleaned.split())
                    return cleaned
                
                cleaned_end_phrase = clean_text(end_phrase)
                
                # è¯­éŸ³è¯†åˆ« - éº¦å…‹é£æ¨¡å¼ç›´æ¥è¿”å›ç»“æœè€Œéç”Ÿæˆå™¨
                asr_result = self.sdk.asr(
                    provider=llm_provider,
                    mode="microphone",
                    duration=duration,
                    silence_timeout=silence_timeout
                )
                if asr_result.get('success'):
                    user_input = asr_result.get('text', '')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸå‘½ä»¤
                cleaned_input = clean_text(user_input)
                if end_phrase and cleaned_input == cleaned_end_phrase:
                    if verbose:
                        logger.info("æ£€æµ‹åˆ°ç»“æŸå¯¹è¯æŒ‡ä»¤ï¼Œæ­£åœ¨ç»“æŸä¼šè¯...")
                    print("ğŸ‘‹ å·²æ£€æµ‹åˆ°ç»“æŸå¯¹è¯æŒ‡ä»¤ï¼Œæ­£åœ¨ç»“æŸä¼šè¯...")
                    conversation_active = False
                    break
                
                if not user_input:
                    print("â“ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³è¾“å…¥ï¼Œè¯·å†è¯•ä¸€æ¬¡")
                    continue
                
                if verbose:
                    logger.info(f"å·²è¯†åˆ«: '{user_input}'")
                print(f"ğŸ” å·²è¯†åˆ«: '{user_input}'")
                
                # å¤šæ¨¡æ€å¤„ç†
                print("ğŸ¤– AIåˆ†æä¸­...")
                
                # æ ¹æ®åª’ä½“ç±»å‹è°ƒç”¨ä¸åŒçš„æ–¹æ³•
                if image_path:
                    multimodal_result = self.sdk.smart_multimodal_chat(
                        prompt=user_input,
                        image_path=image_path,
                        multimodal_provider=llm_provider,
                        multimodal_model=llm_model,
                        tts_provider=tts_provider,
                        tts_model=tts_model,
                        tts_mode="speaker",
                        use_context=use_context,
                        session_id=session_id,
                        **tts_kwargs
                    )
                elif video_path:
                    multimodal_result = self.sdk.smart_multimodal_chat(
                        prompt=user_input,
                        video_path=video_path,
                        multimodal_provider=llm_provider,
                        multimodal_model=llm_model,
                        tts_provider=tts_provider,
                        tts_model=tts_model,
                        tts_mode="speaker",
                        use_context=use_context,
                        session_id=session_id,
                        **tts_kwargs
                    )
                elif image_paths:
                    multimodal_result = self.sdk.smart_multimodal_chat(
                        prompt=user_input,
                        image_paths=image_paths,
                        multimodal_provider=llm_provider,
                        multimodal_model=llm_model,
                        tts_provider=tts_provider,
                        tts_model=tts_model,
                        tts_mode="speaker",
                        use_context=use_context,
                        session_id=session_id,
                        **tts_kwargs
                    )
                
                # è®°å½•å¯¹è¯
                if multimodal_result and multimodal_result.get('success'):
                    ai_response = multimodal_result.get('answer', '')
                    conversation_record = {
                        "user_input": user_input,
                        "ai_response": ai_response
                    }
                    result["conversations"].append(conversation_record)
                    
                    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºå®Œæ•´çš„å›ç­”æ–‡æœ¬
                    print(f"\nâœ“ AIå›ç­”: {ai_response}")
                    print("\nâœ“ å›ç­”å®Œæˆ")
                else:
                    error_msg = multimodal_result.get('error', 'æœªçŸ¥é”™è¯¯') if multimodal_result else 'å¤„ç†å¤±è´¥'
                    print(f"âŒ å¤šæ¨¡æ€å¤„ç†é”™è¯¯: {error_msg}")
                
                # å¦‚æœä¸æ˜¯æŒç»­å¯¹è¯æ¨¡å¼ï¼Œé€€å‡ºå¾ªç¯
                if not continue_conversation:
                    conversation_active = False
                else:
                    # æ¿€æ´»å¤„ç†é€»è¾‘
                    waiting_for_activation = activation_phrase is not None and (not activate_once or not activated_once)
                    if waiting_for_activation:
                        print("ç­‰å¾…ä¸‹ä¸€æ¬¡æ¿€æ´»...")
                    else:
                        print("å‡†å¤‡ä¸‹ä¸€è½®å¯¹è¯...")
                        
        except KeyboardInterrupt:
            if verbose:
                logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œç»“æŸå¯¹è¯")
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç»“æŸå¯¹è¯")
        except Exception as e:
            if verbose:
                logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            result = {"success": False, "error": str(e), "conversations": result.get("conversations", [])}
        
        print("ğŸ™ï¸ğŸ–¼ï¸ æ™ºèƒ½å¤šæ¨¡æ€è¯­éŸ³å¯¹è¯å·²ç»“æŸ")
        return result